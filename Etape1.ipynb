{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Chargement des données"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "3f55cdd735caf277"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "X_train = np.load('/kaggle/input/classer-le-text/data_train.npy')\n",
    "X_test = np.load('/kaggle/input/classer-le-text/data_test.npy')\n",
    "df = pd.read_csv('/kaggle/input/classer-le-text/label_train.csv')\n",
    "y_train = df['label'].to_numpy()"
   ],
   "metadata": {
    "collapsed": true
   },
   "id": "initial_id",
   "execution_count": 0
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Régression logistique de base "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61d5a8ee29dcd840"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "        # Initialisation avec le taux d'apprentissage et le nombre d'itérations\n",
    "        self.learning_rate = learning_rate  \n",
    "        self.iterations = iterations        \n",
    "        self.W = None   \n",
    "        self.b = None    \n",
    "\n",
    "    def sigmoid(self, z):\n",
    "        # Fonction d'activation sigmoid qui transforme les valeurs en probabilités entre 0 et 1\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Entraînement du modèle sur les données X avec les labels y\n",
    "        self.n, self.d = X.shape          # n = nombre d'échantillons, d = nombre de features\n",
    "        self.W = np.zeros(self.d)         # Initialisation des poids à zéro\n",
    "        self.b = 0                        # Initialisation du biais à zéro\n",
    "\n",
    "        for _ in range(self.iterations):\n",
    "            self.update_weights(X, y)      # Mise à jour des poids à chaque itération\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def update_weights(self, X, y):\n",
    "        # Calcul des prédictions actuelles avec la fonction sigmoid\n",
    "        z = self.sigmoid(X.dot(self.W) + self.b)\n",
    "        \n",
    "        # Calcul des gradients pour les poids et le biais\n",
    "        # dW = dérivée partielle par rapport aux poids\n",
    "        dW = (1 / self.n) * np.dot(X.T, (z - y))\n",
    "        # db = dérivée partielle par rapport au biais\n",
    "        db = (1 / self.n) * np.sum(z - y)\n",
    "\n",
    "        # Mise à jour des poids et du biais vec la descente de gradient\n",
    "        self.W -= self.learning_rate * dW\n",
    "        self.b -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Prédiction pour de nouvelles données\n",
    "        z = self.sigmoid(X.dot(self.W) + self.b)\n",
    "        return [1 if i > 0.5 else 0 for i in z]\n",
    "    \n",
    "model = LogisticRegression(learning_rate=0.01, iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "    'ID': range(len(y_pred)),\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "predictions.to_csv('predictions.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e05a5d93a3f2f020"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Pour tester localement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8b20b0e2d17f440e"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "model = LogisticRegression(learning_rate=0.01, iterations=1000)\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_val)\n",
    "\n",
    "macro_f1 = f1_score(y_val, y_pred, average='macro')\n",
    "print(f\"Validation Macro F1 Score: {macro_f1}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "a3d68154cfa11292"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Régression logistique avec normalisation\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b6a05c9feb4fafeb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "   def __init__(self, learning_rate=0.01, iterations=1000):\n",
    "       # Initialisation des paramètres \n",
    "       self.learning_rate = learning_rate  \n",
    "       self.iterations = iterations     \n",
    "       self.W = None      \n",
    "       self.b = None                      \n",
    "       self.mean = None    \n",
    "       self.std = None   \n",
    "\n",
    "   def sigmoid(self, z):\n",
    "       return 1 / (1 + np.exp(-z))\n",
    "\n",
    "   def normalize_features(self, X):\n",
    "       # Normalisation des features (standardisation)\n",
    "       if self.mean is None or self.std is None:\n",
    "           self.mean = np.mean(X, axis=0)  # Moyenne par colonne\n",
    "           self.std = np.std(X, axis=0)    # Écart-type par colonne\n",
    "       # Application de la normalisation\n",
    "       # 1e-8 est ajouté pour éviter la division par zéro\n",
    "       return (X - self.mean) / (self.std + 1e-8)\n",
    "   \n",
    "   def fit(self, X, y):\n",
    "       # Entraînement du modèle\n",
    "       X_normalized = self.normalize_features(X)  # Normalisation des données\n",
    "       self.n, self.d = X_normalized.shape       # n = nb échantillons, d = nb features\n",
    "       self.W = np.zeros(self.d)                 # Initialisation des poids à zéro\n",
    "       self.b = 0                                # Initialisation du biais à zéro\n",
    "\n",
    "       for _ in range(self.iterations):\n",
    "           self.update_weights(X_normalized, y)\n",
    "\n",
    "   def update_weights(self, X, y):\n",
    "       # Calcul des prédictions actuelles\n",
    "       z = self.sigmoid(X.dot(self.W) + self.b)\n",
    "       \n",
    "       # Calcul des gradients\n",
    "       dW = (1 / self.n) * np.dot(X.T, (z - y))  # Gradient pour les poids\n",
    "       db = (1 / self.n) * np.sum(z - y)         # Gradient pour le biais\n",
    "\n",
    "       # Mise à jour des paramètres par descente de gradient\n",
    "       self.W -= self.learning_rate * dW\n",
    "       self.b -= self.learning_rate * db\n",
    "\n",
    "   def predict(self, X):\n",
    "       # Prédiction pour de nouvelles données\n",
    "       X_normalized = self.normalize_features(X)  # Normalisation des nouvelles données\n",
    "       z = self.sigmoid(X_normalized.dot(self.W) + self.b) \n",
    "       return [1 if i > 0.5 else 0 for i in z]\n",
    "   \n",
    "model = LogisticRegression(learning_rate=0.01, iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "    'ID': range(len(y_pred)),\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "predictions.to_csv('predictions1.csv', index=False)\n",
    "\n",
    "# On peut utiliser le même code précédent pour tester localement"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e1436e56c6567b50"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Régression logistique avec normalisation des données et régularisation L2  "
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b937a30366c3dd35"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class LogisticRegression:\n",
    "   def __init__(self, learning_rate=0.01, iterations=1000, lambda_reg=0.01):\n",
    "       self.learning_rate = learning_rate  \n",
    "       self.iterations = iterations       \n",
    "       self.lambda_reg = lambda_reg       \n",
    "       self.W = None                      \n",
    "       self.b = None               \n",
    "       self.mean = None        \n",
    "       self.std = None               \n",
    "\n",
    "   def sigmoid(self, z):\n",
    "       return 1 / (1 + np.exp(-z))\n",
    "\n",
    "   def normalize_features(self, X):\n",
    "       # Normalisation des features\n",
    "       if self.mean is None or self.std is None:\n",
    "           self.mean = np.mean(X, axis=0)\n",
    "           self.std = np.std(X, axis=0)\n",
    "       return (X - self.mean) / (self.std + 1e-8)  \n",
    "   \n",
    "   def fit(self, X, y):\n",
    "       # Entraînement du modèle\n",
    "       X_normalized = self.normalize_features(X)\n",
    "       self.n, self.d = X_normalized.shape\n",
    "       self.W = np.zeros(self.d)\n",
    "       self.b = 0\n",
    "       \n",
    "       self.losses = []  # Stockage des pertes pour suivre la convergence\n",
    "       \n",
    "       for _ in range(self.iterations):\n",
    "           self.update_weights(X_normalized, y)\n",
    "       \n",
    "       return self\n",
    "   \n",
    "   def update_weights(self, X, y):\n",
    "       # Calcul des prédictions\n",
    "       z = self.sigmoid(X.dot(self.W) + self.b)\n",
    "\n",
    "       # Calcul de la fonction de perte (cross-entropy avec régularisation L2)\n",
    "       # 1e-15 ajouté pour éviter log(0)\n",
    "       loss = -np.mean(y * np.log(z + 1e-15) + \n",
    "                     (1 - y) * np.log(1 - z + 1e-15))\n",
    "       # Terme de régularisation L2\n",
    "       reg_loss = self.lambda_reg * np.sum(self.W ** 2) / (2 * self.n)\n",
    "       # Perte totale\n",
    "       total_loss = loss + reg_loss\n",
    "       self.losses.append(total_loss)  # Sauvegarde de la perte\n",
    "\n",
    "       # Calcul des gradients avec régularisation\n",
    "       dW = (1 / self.n) * np.dot(X.T, (z - y)) + \\\n",
    "            (self.lambda_reg * self.W / self.n)  # Gradient régularisé\n",
    "       db = (1 / self.n) * np.sum(z - y)\n",
    "\n",
    "       # Mise à jour des paramètres\n",
    "       self.W -= self.learning_rate * dW\n",
    "       self.b -= self.learning_rate * db\n",
    "       \n",
    "   def predict(self, X):\n",
    "       X_normalized = self.normalize_features(X)\n",
    "       z = self.sigmoid(X_normalized.dot(self.W) + self.b)\n",
    "       return [1 if i > 0.5 else 0 for i in z]\n",
    "   \n",
    "model = LogisticRegression(learning_rate=0.01, iterations=1000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "predictions = pd.DataFrame({\n",
    "    'ID': range(len(y_pred)),\n",
    "    'label': y_pred\n",
    "})\n",
    "\n",
    "predictions.to_csv('predictions2.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-14T02:47:35.052010Z",
     "start_time": "2024-11-14T02:47:31.653768Z"
    }
   },
   "id": "c7c7972d6b4bef04",
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
